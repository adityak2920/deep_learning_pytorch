{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning of CNN through different layers\n",
    "![Different layers of CNN](../img/cnn_layers.png)\n",
    "\n",
    "        The above pictures are of feature maps generated from different layers of VGG16(Visual Geometry Group of Oxford) trained on imagenet.\n",
    "        The Layer 1 generates mostly horizontal, vetical and diagonal lines. There mostly used for detecting edges in an image. The Layer 2 will try to give more informations than first. It detects the corners. The CNN learns to do this on its own. There is no special instruction for the CNN to focus on more complex objects in deeper layers. That’s just how it normally works out when you feed training data into a CNN. Layer 3 is where we start to see some complex patterns like the eyes, face etc. We can assume that this feature maps are obtained from a model trained for detection of human faces. In Layer 4 we see our features finding patterns in the more complex parts of the faces such as eyes.\n",
    "![5th layer of VGG16](../img/layer5.png)\n",
    "        \n",
    "        In Layer 5, you can the feature map generates the specific faces of humans, tyres of cars, faces of animals etc. This feature map contains to most information about the patters found in the images.\n",
    "        \n",
    "### Different Parts of a CNN\n",
    "![Different Parts of CNN](../img/cnn_parts.png)\n",
    "\n",
    "    Now, we have learnt a lot about CNNs, it's time to implement it using PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a CNN Classification Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('data/mnist.csv').values\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 1, 28, 28) (8000,)\n",
      "(1500, 1, 28, 28) (1500,)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping our data according to our CNN\n",
    "xtrain=ds[2000:10000, 1:].reshape((-1, 1, 28, 28))/255.0\n",
    "ytrain=ds[2000:10000, 0]\n",
    "\n",
    "xtest=ds[23000:24500, 1:].reshape((-1, 1, 28, 28))/255.0\n",
    "ytest=ds[23000:24500, 0]\n",
    "\n",
    "print(xtrain.shape, ytrain.shape)\n",
    "print(xtest.shape, ytest.shape) # batch_size, no_of_channels, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Here a question comes, why we divided image by 255 or why it's necessary?\n",
    "Ans:-\n",
    "\n",
    "         These are all scaling techniques, the pixel values are small (Note that these small values still represents the original image), and hence the computation required and time to converge the model reduces significantly. CNN will converge despite taking 0–255 as inputs instead of scaled down to 0 -1 . However, it will converge very slowly.\n",
    "## Data Normalization\n",
    "        Data normalization is an important step which ensures that each input parameter (pixel, in this case) has a similar data distribution. This makes convergence faster while training the network. Data normalization is done by subtracting the mean from each pixel and then dividing the result by the standard deviation. The distribution of such data would resemble a Gaussian curve centered at zero. For image inputs we need the pixel numbers to be positive, so we might choose to scale the normalized data in the range [0,1] or [0, 255].\n",
    "        In PyTorch Normalisation is done, channel-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transforms.Normalize??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "# Checking Output Values or labels\n",
    "print(set(ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):     # inherited methods from super-class nn.Module\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()     # super() is used for calling init method of superclass and use in subclass CNN.\n",
    "                                        #  super() allows to call methods of the superclass in subclass.\n",
    "        self.conv1= nn.Sequential(\n",
    "                nn.Conv2d(1, 16, 5, 1, 2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2))\n",
    "        \n",
    "        self.conv2=nn.Sequential(\n",
    "                nn.Conv2d(16, 32, 5, 1, 2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2))\n",
    "        \n",
    "        self.out=nn.Linear(32*7*7, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "#         print(x.size())\n",
    "        x=x.view(x.size(0), -1)\n",
    "#         print(x.size())\n",
    "        output=F.softmax(self.out(x))\n",
    "#         print(output.size())\n",
    "        return output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Conv2d??\n",
    "# nn.MaxPool2d??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.0001)   \n",
    "\n",
    "loss_func = nn.CrossEntropyLoss() # Also known as classification loss.\n",
    "# L = -y.log(y^) here y is ground-truth and y^ is estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(train, labels, batch=32):\n",
    "    start=0\n",
    "    stop=start+batch\n",
    "    while start<train.shape[0]:\n",
    "        yield torch.FloatTensor(train[start:stop]), torch.LongTensor(labels[start:stop])\n",
    "        start=stop\n",
    "        stop=start+batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/humonics/.virtualenvs/yolo/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | Acc:  92.66666666666667\n",
      "Epoch:  1 | Step:  0 | Acc:  92.66666666666667\n",
      "Epoch:  2 | Step:  0 | Acc:  92.93333333333334\n",
      "Epoch:  3 | Step:  0 | Acc:  93.2\n",
      "Epoch:  4 | Step:  0 | Acc:  93.26666666666667\n",
      "Epoch:  5 | Step:  0 | Acc:  93.6\n",
      "Epoch:  6 | Step:  0 | Acc:  93.66666666666667\n",
      "Epoch:  7 | Step:  0 | Acc:  93.8\n",
      "Epoch:  8 | Step:  0 | Acc:  94.0\n",
      "Epoch:  9 | Step:  0 | Acc:  94.33333333333333\n",
      "Epoch:  10 | Step:  0 | Acc:  94.33333333333333\n",
      "Epoch:  11 | Step:  0 | Acc:  94.6\n",
      "Epoch:  12 | Step:  0 | Acc:  94.53333333333333\n",
      "Epoch:  13 | Step:  0 | Acc:  94.33333333333333\n",
      "Epoch:  14 | Step:  0 | Acc:  94.4\n",
      "Epoch:  15 | Step:  0 | Acc:  94.53333333333333\n",
      "Epoch:  16 | Step:  0 | Acc:  94.86666666666666\n",
      "Epoch:  17 | Step:  0 | Acc:  95.0\n",
      "Epoch:  18 | Step:  0 | Acc:  95.0\n",
      "Epoch:  19 | Step:  0 | Acc:  95.06666666666666\n",
      "Epoch:  20 | Step:  0 | Acc:  95.2\n",
      "Epoch:  21 | Step:  0 | Acc:  95.2\n",
      "Epoch:  22 | Step:  0 | Acc:  95.2\n",
      "Epoch:  23 | Step:  0 | Acc:  95.06666666666666\n",
      "Epoch:  24 | Step:  0 | Acc:  95.26666666666667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-7ba9f949e431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# clear gradients for this training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# for params in cnn.parameters():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/yolo/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/yolo/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for step, (b_x, b_y) in enumerate(make_batch(xtrain, ytrain, 32)):   # gives batch data, normalize x when iterate train_loader\n",
    "        #print (step,)\n",
    "        \n",
    "#         print(b_x.size())\n",
    "        output = cnn(b_x)  \n",
    "#         print(b_y.size())\n",
    "#         cnn output\n",
    "#         print output.size(), output.sum(dim=0)\n",
    "        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        \n",
    "        # for params in cnn.parameters():\n",
    "        #     print params.grad.cpu().data.sum() # Y U no train!!!\n",
    "        optimizer.step()                # apply gradients\n",
    "\n",
    "        if step % 250 == 0:\n",
    "            test_output = cnn(torch.FloatTensor(xtest))\n",
    "            outs = test_output.data.numpy().argmax(axis=1)\n",
    "            acc = (outs == ytest).sum()*100.0 / test_output.shape[0]\n",
    "            # pred_y = torch.max(test_output, 1)[1].data.squeeze().numpy()\n",
    "            # accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0))\n",
    "            print('Epoch: ', epoch, '| Step: ', step, '| Acc: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.0.weight', 'conv1.0.bias', 'conv2.0.weight', 'conv2.0.bias', 'out.weight', 'out.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(cnn.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "for i in cnn.named_parameters():\n",
    "    print(i[1].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Optimizer.state_dict of Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    weight_decay: 0\n",
       ")>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architectures\n",
    "    1. LeNet-5(1998, Yann LeCun)(60,000 parameters)\n",
    "    2. AlexNet(2012, SuperVision group consisting of Alex Krizhevsky, Geoffrey Hinton, and Ilya Sutskever)\n",
    "        It consisted 11x11, 5x5,3x3, convolutions, max pooling, dropout, data augmentation, ReLU activations, SGD with momentum. It attached ReLU activations after every convolutional and fully-connected layer. AlexNet was trained for 6 days simultaneously on two Nvidia Geforce GTX 580 GPUs which is the reason for why their network is split into two pipelines. (60 M parameters)\n",
    "    3. VGG(2012, Visual Geometry Group, Oxford)(528 M for VGG16)\n",
    "    4. Inception(2014, Google)\n",
    "    5. ResNet\n",
    "    6. ResNext\n",
    "    7. EfficientNet\n",
    "    \n",
    "Links to read further about architectures:-\n",
    "\n",
    "    https://towardsdatascience.com/cnn-architectures-a-deep-dive-a99441d18049\n",
    "    https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5\n",
    "    https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "vgg = torchvision.models.vgg16_bn()\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "    The term “dropout” refers to dropping out units (both hidden and visible) in a neural network. I mean these units are not considered during a particular forward or backward pass. More technically, At each training stage, individual nodes are either dropped out of the net with probability 1-p or kept with probability p, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. \n",
    "![Dropout](../img/dropout.png)\n",
    "\n",
    "### Why do we need Dropout?\n",
    "    The answer to this question is “to prevent over-fitting”. A fully connected layer occupies most of the parameters, and hence, neurons develop co-dependency amongst each other during training which curbs the individual power of each neuron leading to over-fitting of training data.\n",
    "    \n",
    "    Effects and Observations of Dropout:-\n",
    "        1. Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n",
    "        2. Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less.\n",
    "        \n",
    "        \n",
    "## Batch Normalization\n",
    "![Batch Normalisation](../img/batch_norm.png)   \n",
    "    \n",
    "    Batch Norm is a technique that aims to improve the training of deep neural networks by stabilizing the distribution of layer inputs. Batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. \n",
    "    Even if you don’t need to worry about overfitting there are many benefits to implementing batch normalization. Because of this, and its regularizing effect, batch normalization has largely replaced dropout in modern convolutional architectures. Because:-\n",
    "    First, dropout is generally less effective at regularizing convolutional layers. The reason? Since convolutional layers have few parameters, they need less regularization to begin with. Furthermore, because of the spatial relationships encoded in feature maps, activations can become highly correlated. This renders dropout ineffective. (Source.)\n",
    "    Second, what dropout is good at regularizing is becoming outdated. Large models like VGG16 included fully connected layers at the end of the network. For models like this, overfitting was combatted by including dropout between fully connected layers.\n",
    "    \n",
    "## So, why Batch-Norm is so effective?\n",
    "    According to those who introduced Batch-Norm, It's success is due to the reduction of ICS(the change in the distribution of layer inputs caused by updating the layer inputs of preceding layers). To explain covariance shift, let’s have a deep network on cat detection. We train our data on only black cats’ images. So, if we now try to apply this network to data with colored cats, it is obvious; we’re not going to do well. The training set and the prediction set are both cats’ images but they differ a little bit. In other words, if an algorithm learned some X to Y mapping, and if the distribution of X changes, then we might need to retrain the learning algorithm by trying to align the distribution of X with the distribution of Y. \n",
    "    \n",
    "    But Actually the performance is improved in CNNs due to Batch-Norm beacuse of other reason:-\n",
    "According to Ian Goodfellow:-\n",
    "\n",
    "        Deep Neural networks have higher-order interactions, which means changing weights of one layer might also effect the statistics of other layers in addition to the loss function. These cross layer interactions, when unaccounted lead to internal covariate shift. Every time we update the weights of a layer, there is a chance that it effects the statistics of a layer further in the neural network in an unfavorable way.\n",
    "\n",
    "    Convergence may require careful initializing, hyperparameter tuning and longer training durations in such cases. However, when we add the batch normalized layer between the layers, the statistics of a layer are only effected by the two hyperparameters γ and β. Now our optimization algorithm has to adjust only two hyperparameters to control the statistics of any layer, rather than the entire weights in the previous layer. This greatly speeds up convergence, and avoids the need for careful initialization and hyperparameter tuning. Therefore, Batch Norm acts more like a check pointing mechanism.\n",
    "    \n",
    "According to paper published by MIT:-\n",
    " \n",
    "![Effect of Batch Norm](../img/effect_batch.png)\n",
    "    \n",
    "    It reparametrize the underlying optimization problem to make its loss landscape significantly more smooth. That is, the loss changes at a smaller rate and the magnitudes of the gradients are smaller too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
