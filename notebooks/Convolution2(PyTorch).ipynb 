{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('/Users/adityakumar/Desktop/ML/train.csv').values\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import torch\n",
    "\n",
    "batch_size = 5\n",
    "nb_digits = 10\n",
    "# Dummy input that HAS to be 2D for the scatter (you can use view(-1,1) if needed)\n",
    "y = torch.LongTensor(batch_size,1).random_() % nb_digits\n",
    "# One hot encoding buffer that you create out of the loop and just keep reusing\n",
    "y_onehot = torch.FloatTensor(batch_size, nb_digits)\n",
    "\n",
    "# In your for loop\n",
    "y_onehot.zero_()\n",
    "y_onehot.scatter_(1, y, 2)\n",
    "y_onehot.scatter_?\n",
    "\n",
    "print(y)\n",
    "print(y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_hot(data, n_classes=None):\n",
    "#     if n_classes is None:\n",
    "#         n_classes = np.unique(data).shape[0]\n",
    "#     oh = np.zeros((len(data), n_classes))\n",
    "\n",
    "#     for ix in range(oh.shape[0]):\n",
    "#         oh[ix, data[ix]] = 1\n",
    "#     return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot('aditya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 1, 28, 28) (8000,)\n",
      "(1500, 1, 28, 28) (1500,)\n"
     ]
    }
   ],
   "source": [
    "xtrain=ds[2000:10000, 1:].reshape((-1, 1, 28, 28))/255.0\n",
    "ytrain=ds[2000:10000, 0]\n",
    "\n",
    "xtest=ds[23000:24500, 1:].reshape((-1, 1, 28, 28))/255.0\n",
    "ytest=ds[23000:24500, 0]\n",
    "\n",
    "print(xtrain.shape, ytrain.shape)\n",
    "print(xtest.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1= nn.Sequential(\n",
    "                nn.Conv2d(1, 16, 5, 1, 2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2))\n",
    "        \n",
    "        self.conv2=nn.Sequential(\n",
    "                nn.Conv2d(16, 32, 5, 1, 2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2))\n",
    "        \n",
    "        self.out=nn.Linear(32*7*7, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=x.view(x.size(0), -1)\n",
    "        \n",
    "        output=F.softmax(self.out(x))\n",
    "        return output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.0001)   \n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myrange(t):\n",
    "    k = 0\n",
    "    while k < t:\n",
    "        yield k\n",
    "        k += 1\n",
    "z = myrange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(train, labels, batch=32):\n",
    "    start=0\n",
    "    stop=start+batch\n",
    "    while start<train.shape[0]:\n",
    "        yield Variable(torch.FloatTensor(train[start:stop])), Variable(torch.LongTensor(labels[start:stop]))\n",
    "        start=stop\n",
    "        stop=start+batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | Acc:  11.066666666666666\n",
      "Epoch:  1 | Step:  0 | Acc:  71.86666666666666\n",
      "Epoch:  2 | Step:  0 | Acc:  79.2\n",
      "Epoch:  3 | Step:  0 | Acc:  86.93333333333334\n",
      "Epoch:  4 | Step:  0 | Acc:  88.0\n",
      "Epoch:  5 | Step:  0 | Acc:  88.73333333333333\n",
      "Epoch:  6 | Step:  0 | Acc:  89.4\n",
      "Epoch:  7 | Step:  0 | Acc:  89.93333333333334\n",
      "Epoch:  8 | Step:  0 | Acc:  90.26666666666667\n",
      "Epoch:  9 | Step:  0 | Acc:  90.53333333333333\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for step, (b_x, b_y) in enumerate(make_batch(xtrain, ytrain, 32)):   # gives batch data, normalize x when iterate train_loader\n",
    "        #print (step,)\n",
    "        output = cnn(b_x)               # cnn output\n",
    "        # print output.size(), output.sum(dim=0)\n",
    "        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        \n",
    "        # for params in cnn.parameters():\n",
    "        #     print params.grad.cpu().data.sum() # Y U no train!!!\n",
    "        optimizer.step()                # apply gradients\n",
    "\n",
    "        if step % 250 == 0:\n",
    "            test_output = cnn(torch.FloatTensor(xtest))\n",
    "            outs = test_output.data.numpy().argmax(axis=1)\n",
    "            acc = (outs == ytest).sum()*100.0 / test_output.shape[0]\n",
    "            # pred_y = torch.max(test_output, 1)[1].data.squeeze().numpy()\n",
    "            # accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0))\n",
    "            print('Epoch: ', epoch, '| Step: ', step, '| Acc: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MaxPool2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "outs = cnn(torch.FloatTensor(xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.0875"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(outs.data.numpy().argmax(axis=1) == ytrain).sum()*100 / float(ytrain.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.0.weight', 'conv1.0.bias', 'conv2.0.weight', 'conv2.0.bias', 'out.weight', 'out.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(cnn.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0957, -0.1919, -0.2693, -0.1193, -0.1435],\n",
      "          [ 0.0932, -0.0897, -0.0934,  0.1189,  0.0421],\n",
      "          [ 0.3075,  0.3253,  0.0100,  0.1030, -0.0904],\n",
      "          [ 0.3247,  0.2990,  0.1200,  0.3255,  0.2987],\n",
      "          [ 0.1592,  0.1073,  0.1253,  0.1557, -0.0701]]],\n",
      "\n",
      "\n",
      "        [[[-0.2096,  0.2220,  0.0036,  0.2998,  0.1053],\n",
      "          [-0.0360, -0.0701,  0.2461, -0.1694, -0.2011],\n",
      "          [-0.1027, -0.0776,  0.3585, -0.2045, -0.3748],\n",
      "          [ 0.0485,  0.3410,  0.0762,  0.1664,  0.0253],\n",
      "          [ 0.0261,  0.4141,  0.4132,  0.1366,  0.1713]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3088,  0.0797,  0.0150, -0.2087,  0.1157],\n",
      "          [ 0.0782,  0.3036,  0.2360, -0.3570, -0.1697],\n",
      "          [ 0.2879,  0.0710,  0.1518, -0.3212, -0.3312],\n",
      "          [ 0.3016,  0.0674,  0.1285, -0.3198, -0.2706],\n",
      "          [ 0.3703,  0.0333,  0.2575, -0.1406, -0.1943]]],\n",
      "\n",
      "\n",
      "        [[[-0.1046,  0.2670,  0.0047, -0.1261,  0.0739],\n",
      "          [ 0.2063,  0.1767,  0.2246,  0.1719, -0.0698],\n",
      "          [ 0.2564,  0.1397,  0.3335,  0.0443, -0.0590],\n",
      "          [-0.1984,  0.0263,  0.1649,  0.2055,  0.0409],\n",
      "          [ 0.1296, -0.1170,  0.2168,  0.2655,  0.0391]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3210,  0.1182,  0.1692,  0.2686,  0.1932],\n",
      "          [ 0.1633,  0.2879,  0.3119,  0.3543,  0.1381],\n",
      "          [ 0.1244, -0.0542,  0.0264,  0.1596,  0.0410],\n",
      "          [-0.1815, -0.3207, -0.3691, -0.4288, -0.2455],\n",
      "          [-0.3775, -0.3907, -0.2896, -0.0657, -0.1844]]],\n",
      "\n",
      "\n",
      "        [[[-0.1433, -0.1454, -0.1104, -0.4156, -0.2383],\n",
      "          [-0.1974, -0.4425, -0.3940, -0.0297,  0.1172],\n",
      "          [-0.0656, -0.0275, -0.1044, -0.0346,  0.3150],\n",
      "          [ 0.2030,  0.3542,  0.1042,  0.2726,  0.0521],\n",
      "          [ 0.2224,  0.4179,  0.3679,  0.3426,  0.0797]]],\n",
      "\n",
      "\n",
      "        [[[-0.1099,  0.1371,  0.2629,  0.0461, -0.1646],\n",
      "          [ 0.0650,  0.3318,  0.0113, -0.0287, -0.1287],\n",
      "          [ 0.1369,  0.2885,  0.3521, -0.0033,  0.2552],\n",
      "          [ 0.0644,  0.1249,  0.1618,  0.3034,  0.2101],\n",
      "          [ 0.0936, -0.0794, -0.0827, -0.0324,  0.2426]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2997,  0.0262,  0.2519,  0.1188,  0.2955],\n",
      "          [ 0.1676,  0.3375,  0.2206,  0.2910, -0.0882],\n",
      "          [ 0.2141,  0.0725,  0.2592,  0.2647,  0.0829],\n",
      "          [ 0.0350, -0.1209, -0.0312, -0.1782, -0.0351],\n",
      "          [ 0.0479, -0.0601,  0.1964, -0.0043, -0.1291]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1670,  0.1095,  0.1410,  0.0712, -0.3277],\n",
      "          [ 0.2969,  0.1359, -0.0274, -0.2071, -0.2225],\n",
      "          [ 0.3613,  0.2293, -0.1241, -0.1704, -0.4044],\n",
      "          [ 0.0766, -0.0297,  0.1566, -0.0846, -0.4073],\n",
      "          [ 0.0809,  0.1729, -0.0126, -0.1004, -0.3376]]],\n",
      "\n",
      "\n",
      "        [[[-0.0767,  0.1808,  0.2833,  0.3749,  0.3475],\n",
      "          [-0.3008, -0.0089,  0.0116,  0.2822,  0.0166],\n",
      "          [-0.3788,  0.0245,  0.2006,  0.1148,  0.0193],\n",
      "          [-0.2114,  0.2117,  0.2153, -0.0460, -0.1652],\n",
      "          [-0.1916,  0.2532, -0.0546, -0.0349, -0.0089]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2047,  0.2125, -0.0698, -0.1559,  0.1540],\n",
      "          [ 0.1046,  0.2893,  0.3158,  0.1437,  0.0947],\n",
      "          [ 0.1070,  0.4298,  0.2262,  0.2369,  0.1573],\n",
      "          [ 0.2936,  0.2225,  0.0437,  0.1324,  0.2011],\n",
      "          [-0.0099,  0.0818, -0.1824, -0.2948, -0.3393]]],\n",
      "\n",
      "\n",
      "        [[[-0.2003, -0.2017, -0.0791,  0.1750, -0.0960],\n",
      "          [ 0.0249, -0.0683, -0.0940,  0.0093,  0.2146],\n",
      "          [ 0.1989,  0.2058,  0.1052,  0.3997,  0.1553],\n",
      "          [ 0.2567,  0.2214,  0.3949,  0.2863,  0.3257],\n",
      "          [-0.1366,  0.1217,  0.3163,  0.4025,  0.0967]]],\n",
      "\n",
      "\n",
      "        [[[-0.0894,  0.0580, -0.0709,  0.0957,  0.1869],\n",
      "          [-0.3457, -0.3149,  0.0112,  0.1644,  0.2714],\n",
      "          [-0.2734, -0.1214,  0.0048,  0.2889,  0.1158],\n",
      "          [ 0.1843,  0.0349,  0.1225,  0.1133,  0.3355],\n",
      "          [ 0.3192,  0.3004,  0.1485,  0.3418, -0.0882]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1328,  0.2049, -0.1214,  0.1921,  0.0529],\n",
      "          [-0.0773,  0.0207,  0.2566,  0.3399,  0.0113],\n",
      "          [ 0.1851,  0.4100,  0.3057,  0.2035,  0.3636],\n",
      "          [-0.1206,  0.2454,  0.2642,  0.2708,  0.2379],\n",
      "          [-0.3267, -0.2318, -0.1625, -0.0634,  0.0719]]],\n",
      "\n",
      "\n",
      "        [[[-0.0791, -0.1828,  0.1353,  0.2864,  0.0236],\n",
      "          [-0.1717, -0.0486,  0.3016,  0.0782,  0.1451],\n",
      "          [-0.1538,  0.1475, -0.0225,  0.3096,  0.0686],\n",
      "          [ 0.0947,  0.2667,  0.2091,  0.1069,  0.2775],\n",
      "          [-0.0925,  0.2767,  0.3169,  0.3122,  0.2193]]],\n",
      "\n",
      "\n",
      "        [[[-0.0481, -0.0122, -0.1507, -0.3134, -0.2298],\n",
      "          [-0.1925, -0.2783, -0.0751, -0.1458,  0.0817],\n",
      "          [ 0.2819, -0.0967, -0.0290,  0.1356,  0.2813],\n",
      "          [ 0.1343,  0.1494,  0.3492,  0.3855,  0.0904],\n",
      "          [ 0.1349,  0.3178,  0.0637,  0.2079,  0.1321]]]])\n"
     ]
    }
   ],
   "source": [
    "conv_01 = cnn.state_dict()['conv1.0.weight']\n",
    "print(conv_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f1a0828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(0)\n",
    "for ix in range(conv_01.shape[0]):\n",
    "    plt.subplot(4, 4, ix+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(conv_01[ix].reshape((5, 5)), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
