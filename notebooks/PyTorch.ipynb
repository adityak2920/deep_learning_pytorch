{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PyTorch?\n",
    "    PyTorch is a library for Python programs that facilitates building deep learning projects. It provides a core       data structure, the Tensor, a multidimensional array that has many similarities with NumPy arrays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Deep Learning?\n",
    "![Change in perspective brought by deep learning](../img/image.png)\n",
    "    \n",
    "    On the left side of figure, a practitioner is busy defining engineering features and feeding them to a         learning algorithm. The results of the task will be as good as the features he engineers. On the right side of the  figure, with deep learning, the raw data is fed to an algorithm that extracts hierarchical features automatically,  based on optimizing the performance of the algorithm on the task. The results will be as good as the practitioner’s ability to drive the algorithm toward its goal.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why PyTorch?\n",
    "    - It’s Pythonic, using the library generally feels familiar to developers who have used Python previously.\n",
    "    - PyTorch feels like NumPy, but with GPU acceleration and automatic computation of gradients, which makes it       suitable for calculating backward pass data automatically starting from a forward expression.\n",
    "    - PyTorch has been equipped with a high-performance C++ runtime that users can leverage to deploy models for       inference without relying on Python, keeping most of the flexibility of PyTorch without paying the overhead of the Python runtime.\n",
    "    - Benefit of Having Dynamic Computational Graph(Imp).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Computational Graph\n",
    "\n",
    "#### Immediate versus Deferred Execution\n",
    "    By Considering the expression: \n",
    "                    (a**2 + b**2) ** 0.5 (Pythagorean theorem)\n",
    "        \n",
    "        Immediate Execution:\n",
    "                    >>> a = 3\n",
    "                    >>> b = 4\n",
    "                    >>> c = (a**2 + b**2) ** 0.5 \n",
    "                    >>> c\n",
    "                    5.0\n",
    "                    \n",
    "        - Immediate execution like this consumes inputs and produces an output value (c here). PyTorch, like Python in general, defaults to immediate execution (referred to as eager mode in the PyTorch documentation). Immediate execution is useful because if problems arise in executing the expression, the Python interpreter, debugger, and similar tools have direct access to the Python objects involved. Exceptions can be raised directly at the point where the issue occurred.\n",
    "                    \n",
    "        Deferred Execution:\n",
    "                    >>> p = lambda a, b: (a**2 + b**2) ** 0.5 \n",
    "                    >>> p(1, 2)\n",
    "                    2.23606797749979\n",
    "                    >>> p(3, 4)\n",
    "                    5.0\n",
    "        - Deferred execution means that most exceptions are be raised when the function is called, not when it’s defined. For normal Python (as you see here), that’s fine, because the interpreter and debuggers have full access to the Python state at the time when the error occurred.\n",
    "                    \n",
    "\n",
    "                    >>> a = InputParameterPlaceholder() \n",
    "                    >>> b = InputParameterPlaceholder() \n",
    "                    >>> c = (a**2 + b**2) ** 0.5\n",
    "                    >>> callable(c)\n",
    "                    True\n",
    "                    >>> c(3, 4) 5.0                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        - This looks like immediate execution to be deferred under the hood. Often in libraries that use this form of function definition, the operations of squaring a and b, adding, and taking the square root aren’t recorded as high-level Python byte code. Instead, the point usually is to compile the expression into a static computation graph (a graph of basic operations) that has some advantage over pure Python (such as compiling the math directly to machine code for performance reasons)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational Graphs\n",
    "\n",
    "    The fundamental building block of a neural network is a neuron. Neurons are strung together in large numbers to form the network. \n",
    "                    o = tanh(w * x + b)\n",
    "        􏰀 x is the input to the single-neuron computation.\n",
    "        􏰀 w and b are the parameters or weights of the neuron and can be changed as\n",
    "        needed.\n",
    "        􏰀 To update the parameters (to produce output that more closely matches what\n",
    "        we desire), we assign error to each of the weights via backpropagation and then\n",
    "        tweak the weights accordingly.\n",
    "        􏰀 Backpropagation requires computing the gradient of the output with respect to\n",
    "        the weights (among other things).\n",
    "        􏰀 We use automatic differentiation to compute the gradient automatically, saving\n",
    "        us the trouble of writing the calculations by hand.\n",
    "        \n",
    "![Static Computational Graph](../img/static.png)\n",
    "        \n",
    "        This kind of graph uses a similar kind of deferred execution. \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dynamic Computational Graph](../img/dynamic.png)\n",
    "\n",
    "    PyTorch supports a define-by-run dynamic graph engine in which the computation graph is built node by node as  the code is eagerly evaluated.\n",
    "                This does not mean dynamic graph libraries are inherently more capable than static graph libraries,     just that it’s often easier to accomplish looping or conditional behavior with dynamic graphs.\n",
    "                \n",
    "![Dynamic Computational Graph](../img/dynamic_graph.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level map of the main components of PyTorch\n",
    "    First, PyTorch has the Py from Python, but there’s a lot of non-Python code in it. For performance reasons, most of PyTorch is written in C++ and CUDA , a C++-like language from NVIDIA that can be compiled to run with massive parallelism on NVIDIA GPUs. There are ways to run PyTorch directly from C.\n",
    "            \n",
    "            Basic high-level structure of a PyTorch project:\n",
    "![Structure of a PyTorch project](../img/component.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
